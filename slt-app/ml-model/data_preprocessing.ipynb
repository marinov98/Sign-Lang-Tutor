{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASL Handshape Data\n",
    "\n",
    "## Description of data\n",
    "Link to data found on Nicolas Pugeault's website\n",
    "(http://empslocal.ex.ac.uk/people/staff/np331/index.php?section=FingerSpellingDataset). The dataset is of 24 static handshapes corresponding to English letters (excluding the letters \"J\" and \"Z\" since they require motion). The data comprises of 5 different non-native signers of about 60,000 RGB (intensity) images and depth images. The images have some rotational variance as the subject moved their hand during the image capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook setup\n",
    "\n",
    "Run this section if data is already partially processed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image \n",
    "from functools import reduce\n",
    "\n",
    "## Variables needed for reference throughout sections\n",
    "dataset_url = 'www.cvssp.org/FingerSpellingKinect2011/fingerspelling5.tar.bz2'\n",
    "filename = 'fingerspelling5.tar.bz2'\n",
    "# Data directory\n",
    "final_data_dir = 'data'\n",
    "# Data's top-level directory (after download & decompreshion)\n",
    "dataset_dir = 'dataset5'\n",
    "# Filename for image sizes\n",
    "img_sizes_csv = 'image_sizes.csv' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grabbing data\n",
    "\n",
    "Below is a script to download the data to the local machine. Note that compressed file is over 2GB. If the data was already retrieved, you can skip this section and start with the preprocessing the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link to dataset and \n",
    "os.system('wget {URL}'.format(URL=dataset_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncompress\n",
    "os.system('tar xjf {}'.format(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing and relabelling data\n",
    "\n",
    "Only RGB image data is needed and should be relabelled so that the files can be easily be placed into one directory but still contain metadata for classification, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new data directory if doesn't exist\n",
    "if not os.path.exists(final_data_dir):\n",
    "    os.makedirs(final_data_dir)\n",
    "\n",
    "# Define patterns for depth files & RGB files \n",
    "# Format: `depth_0_0528.png` & `color_12_0137.png`\n",
    "pattern_depth_file = '(depth\\w*.png)'\n",
    "pattern_rgb_file = 'color_\\d*_(\\d*).png'\n",
    "# Number of files renamed/delted\n",
    "n_del, n_rename = 0,0\n",
    "\n",
    "# Save that this is a new subject (numerical since letter can be confusing)\n",
    "# Each subject in directory with a letter ('A','B','C',...)\n",
    "for (subject_id, subject_dir) in enumerate(os.listdir(dataset_dir)):\n",
    "    # Directories for each letter (excluding \"j\" & \"z\")\n",
    "    path_to_subject = os.path.join(dataset_dir, subject_dir)\n",
    "    \n",
    "    for letter_dir in os.listdir(path_to_subject):\n",
    "        # Use letter as number ('a' starts @ 00)\n",
    "        letter_id = ord(letter_dir.lower()) - ord(('a'))\n",
    "        letter_id = '0{}'.format(letter_id) if letter_id < 10 else letter_id\n",
    "        path_to_letter = os.path.join(path_to_subject, letter_dir)\n",
    "        \n",
    "        for image_file in os.listdir(path_to_letter):\n",
    "            # Remove depth file\n",
    "            if re.search(pattern_depth_file, image_file):\n",
    "                path_depth_file = os.path.join(path_to_letter, image_file)\n",
    "                os.remove(path_depth_file)\n",
    "                # Inform depth file removed\n",
    "                print('\\r#{}: Depth file deleted {}'.format(n_del,path_depth_file), end='')\n",
    "                n_del += 1\n",
    "            else:\n",
    "                # Get ID of each file (None if not matched)\n",
    "                num_id = re.match(pattern_rgb_file, image_file)\n",
    "                if num_id:\n",
    "                    # Get the matching parathesis only\n",
    "                    num_id = num_id.group(1)\n",
    "                    path_image_file = os.path.join(path_to_letter, image_file)\n",
    "                    # Rename image\n",
    "                    new_image_name = '{}_{}_{}.png'.format(letter_id,subject_id,num_id)\n",
    "                    new_path_image_file = os.path.join(final_data_dir, new_image_name)\n",
    "                    os.rename(path_image_file, new_path_image_file)\n",
    "                    # Inform image renamed\n",
    "                    print('\\r#{}: {} renamed from {}'.format(n_rename,new_path_image_file,path_image_file), end='')\n",
    "                    n_rename += 1\n",
    "            sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring image size\n",
    "\n",
    "The next step would be to make all images the same size, however it is unclear what are the images' current sizes. So a quick exploration will done to determine how the images will be resized (and padded if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create headers for dataframe columns\n",
    "img_dict = {'filename':[], 'width':[], 'height':[]}\n",
    "\n",
    "# Get only relevant files\n",
    "file_list = [x for x in os.listdir(final_data_dir) if re.search('\\w*png', x)]\n",
    "for img_filename in file_list:\n",
    "    # Open up the image file and get relevant info\n",
    "    img_path = os.path.join(final_data_dir, img_filename)\n",
    "    img = Image.open(img_path)\n",
    "    width, height = img.size\n",
    "    # Add it to a dictionary before converting to dataframe\n",
    "    img_dict['filename'].append(img_filename)\n",
    "    img_dict['width'].append(width)\n",
    "    img_dict['height'].append(height)\n",
    "    \n",
    "# Convert to dataframe and save to CSV    \n",
    "img_df = pd.DataFrame(data=img_dict)\n",
    "img_df.to_csv(img_sizes_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in CSV to df if continuing\n",
    "img_df = pd.read_csv(img_sizes_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating some new variables to get better idea of images\n",
    "img_df['area'] = img_df.apply(lambda row: row.width * row.height, axis=1)\n",
    "img_df['aspect'] = img_df.apply(lambda row: row.width / row.height, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = img_df['height'].hist(bins=100)\n",
    "\n",
    "ax.set_title('Distribution of Images\\' Heights')\n",
    "ax.set_xlabel('Height of image (in pixels)')\n",
    "ax.set_ylabel('Number of images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = img_df['width'].hist(bins=100)\n",
    "\n",
    "ax.set_title('Distribution of Images\\' Widths')\n",
    "ax.set_xlabel('Width of image (in pixels)')\n",
    "ax.set_ylabel('Number of images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observing the total pixel area\n",
    "ax = img_df['area'].hist(bins=100)\n",
    "\n",
    "ax.set_title('Distribution of Images\\' Area')\n",
    "ax.set_xlabel('Height of image (in pixels*pixels)')\n",
    "ax.set_ylabel('Number of images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Observing the pixel aspect ratio\n",
    "ax = img_df['aspect'].hist(bins=100)\n",
    "\n",
    "ax.set_title('Distribution of Images\\' Aspect Ratio')\n",
    "ax.set_xlabel('Aspect ratio of image (width / height)')\n",
    "ax.set_ylabel('Number of images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image sizes observations\n",
    "\n",
    "Looking at our images different sizes, one can pick up on some patterns in the data. First the smallest the height or width each goes down is $64$ pixels while the maximums go to over about $270$ and $250$ pixels for height and width respectively. Height is roughly a normal curve centered near $145$ pixels while width is skewed towards smaller values with an average of about $100$ pixels.\n",
    "\n",
    "Aspect ratio and area metrics were also created from the data. The area gives us literally how many inputs there would be for the model if we simply input the pixels. The aspect ratio gives the relative size of height and width.\n",
    "\n",
    "One can notice that the aspect ratio trends below $1.0$ meaning that most images are taller than they are wide. This makes sense since the images likely capture part of the arm which for most signs creates a more vertical image. Less than $25\\%$ of all images have an aspect ratio above the $1.0$ which corresponds with images wider than they are tall.\n",
    "\n",
    "One can also notice that the area metric is heavily skewed toward smaller values. This fits with the aspect ratio and likely is because of the skew in width. The average number of pixels in the images is about $15000$ pixels and less than $25\\%$ have more than $18000$ pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinig image resizing action\n",
    "\n",
    "Taking this information from the dataset, a procedure can be created to resize these images. Since the model will be using a convolutional neural network (CNN) architecture, the images must be fixed and benefits in being square that is the width and height must be equal. Ideally, one would like the images to keep as much information as possible during the resize but due to computing resource limits, the images shouldn't simply enlarged to fit to the maximums. Therefore, in resizing the images most images should retain most (if not all) of their information while there are fewer images losing information. Note that the images should not be cropped since they have already been cropped around the images' handshapes.\n",
    "\n",
    "From the data exploration, it can be observed that most images are taller than wide. It would then make sense that resizing the data should favor taller images since this is more common in the data. One solution is to add padding to images so that the width matches with the height. However, by adding the padding can give bias to the model if particular letters have differing aspect ratios (see the histogram plots below). In other words, the model could simply learn based on the padding instead of the handshape in the image. Thus stretching/squeezing or rescaling the image seems to be preferred even if the images will lose more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the handshape from file name (convert number to letter)\n",
    "img_df['handshape'] = img_df.apply(lambda row: chr(int(row.filename[:2])+65), axis=1)\n",
    "# Plot the histograms by handshape\n",
    "img_df['aspect'].hist(by=img_df['handshape'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that it has been determined that rescaling is the best strategy, it now needs to be determined how large the rescaled images will be for the CNN architecture. It has already been said that a square image is preferred. More than $75\\%$ of the images have aspect ratios less than $1.0$ so most images can be simply rescaled to a square by changing the width. \n",
    "\n",
    "Restricting the width to $125$ pixels would keep information preserved for about $75\\%$ of the images. This seems to be a reasonable for width but height has to be considered as well. Restricting the image to $125$ pixels for height would mean information would be preserved for less than $25\\%$ of the images. This looks to be a poor tradeoff especially considering that most images are tall and therefore most pictures would lose information.\n",
    "\n",
    "Focusing on height instead, a restriction of $160$ pixels would preserve all the height information for about $75\\%$ of the images. Good so far observing how many images would preserve all its width image is over $75\\%$. This is great since most images will retain most (if not all) of their information.\n",
    "  \n",
    "In conclusion, resizing the images to be $160$ pixels by $160$ pixels will preserve most of the images' information. This size is also ideal since each dimension is divisible by $2^5$ which gives us a decently sized power of $2$. The total number of pixels for each image will be $25600$ pixels which is on the higher end but allows for a large proportion of the images to not lose information during the resize. There is a tradeoff between size and computation time so if this appears to be an issue during training, the image can be reduced farther so each dimension will be $134$ pixels giving a total number of pixels of $17956$. This would still keep over $75\\%$ of images to retain most of their information after resizing.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resizing images for model\n",
    "\n",
    "Now that it has been determined what and how the images should be resized, we perform it on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert image to new size and save file\n",
    "def img_resize(width, height, img_file, new_name=None):\n",
    "    img_path = os.path.join(final_data_dir, img_file)\n",
    "    img = Image.open(img_path)\n",
    "    img = img.resize((width, height))\n",
    "    #\n",
    "    if new_name != None:\n",
    "        img_path = os.path.join(final_data_dir, new_name)\n",
    "    img.save(img_path)\n",
    "    \n",
    "\n",
    "# Resize height and width (pixels) (160,160) --> (224,224) for ResNet50\n",
    "new_width, new_height  = 224, 224\n",
    "\n",
    "# Keep track coverts\n",
    "n = 1\n",
    "file_list = [x for x in os.listdir(final_data_dir) if re.search('\\w*png', x)]\n",
    "n_imgs = len(file_list)\n",
    "errors_resize = []\n",
    "errors = 0\n",
    "\n",
    "# Resize all images\n",
    "for img_filename in file_list:\n",
    "    # Keep track of files that were not successfully resized\n",
    "    try:\n",
    "        # Open up the image file and resize\n",
    "        img_resize(new_width, new_height, img_filename)\n",
    "    except:\n",
    "        errors_resize.append((n,img_filename))\n",
    "        errors += 1\n",
    "    # Message out    \n",
    "    print('\\r#{} of {}: Resizing image `{}` w/ {} errors'.format(n,n_imgs,img_filename,errors), end='')   \n",
    "    n += 1\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data\"\n",
    "\n",
    "\n",
    "\n",
    "def move_data_by_category(container_dir, regex_file_format='.*png'):\n",
    "    '''Move data into a directory based on category'''\n",
    "    # Still check if files are images\n",
    "    file_list = [x for x in os.listdir(container_dir) if re.search(regex_file_format, x)]\n",
    "    # Get numerical string (note that 1 digits are represented w/ 2 digits) \n",
    "    letters = {x.split('_')[0] for x in file_list}\n",
    "    \n",
    "    for letter in letters:\n",
    "        # Only images that match letter\n",
    "        images_with_letter = [filename for filename in file_list if filename.split('_')[0] == letter]\n",
    "        # Add images to sub directory\n",
    "        new_categ_path = os.path.join(container_dir, letter)\n",
    "        if not os.path.exists(new_categ_path):\n",
    "            os.makedirs(new_categ_path)\n",
    "#         print('Created {new_categ_path} dir with {len(images_with_letter)} items')\n",
    "        for img_filename in images_with_letter:\n",
    "            path = os.path.join(container_dir, img_filename)\n",
    "            new_path = os.path.join(new_categ_path, img_filename)            \n",
    "            os.rename(path, new_path)\n",
    "    # TODO: Check if any files were skipped (improperly named?)\n",
    "        \n",
    "\n",
    "def get_testing_data(data_dir, subject_num='4'):\n",
    "    '''Get all data/images pertaining to one subject'''\n",
    "    # Only search in directory for images with that subject\n",
    "    file_list = [x for x in os.listdir(data_dir) if re.search('\\d+_{}_\\d*\\.png'.format(subject_num), x)]\n",
    "    \n",
    "    # Make a new testing data directory if doesn't exist\n",
    "    testing_dir = os.path.join(data_dir, 'test')\n",
    "    if not os.path.exists(testing_dir):\n",
    "        os.makedirs(testing_dir)\n",
    "        \n",
    "    # Move images of particular subject into testing directory\n",
    "    for image_filename in file_list:\n",
    "        # file is **_n_****.png where n is an integer representing a subject\n",
    "        _, subject, _ = image_filename.split('_')\n",
    "        # Move file into testing directory\n",
    "        path = os.path.join(data_dir, image_filename)\n",
    "        new_path = os.path.join(testing_dir, image_filename)\n",
    "        os.rename(path, new_path)\n",
    "        \n",
    "    # Move each image file's numerical str representing letters found in testing into own category directory\n",
    "    move_data_by_category(testing_dir)\n",
    "\n",
    "\n",
    "def get_training_validation_data(data_dir, ratio=0.8):\n",
    "    '''Randomly split data into training and validation sets'''\n",
    "    # Only search in directory for images\n",
    "    file_list = [x for x in os.listdir(data_dir) if re.search('.*png', x)]\n",
    "    \n",
    "    # Make a new training & validation data directory if doesn't exist\n",
    "    train_dir = os.path.join(data_dir, 'train')\n",
    "    valid_dir = os.path.join(data_dir, 'valid')\n",
    "    if not os.path.exists(train_dir):\n",
    "        os.makedirs(train_dir)\n",
    "    if not os.path.exists(valid_dir):\n",
    "        os.makedirs(valid_dir)\n",
    "        \n",
    "    # Randomly split file list into training and vaidation sets\n",
    "    np.random.shuffle(file_list)\n",
    "    split_int = int(ratio * len(file_list))\n",
    "    train_list = file_list[:split_int]\n",
    "    valid_list = file_list[split_int:]\n",
    "    \n",
    "    # Move images of particular subject into testing directory\n",
    "    for filenames, new_dir in [(train_list, train_dir), (valid_list, valid_dir)]:\n",
    "        for image_filename in filenames:\n",
    "            # Move file into testing directory\n",
    "            path = os.path.join(data_dir, image_filename)\n",
    "            new_path = os.path.join(new_dir, image_filename)\n",
    "            os.rename(path, new_path)\n",
    "\n",
    "        # Move each image file's numerical str representing letters found in testing into own category directory\n",
    "        move_data_by_category(new_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_testing_data(data_dir)\n",
    "get_training_validation_data(data_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}